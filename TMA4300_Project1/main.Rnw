\documentclass[a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[eng]{babel}
\usepackage{hyperref}
\usepackage{stmaryrd}
\usepackage{algorithm}
\usepackage{amsfonts}
\usepackage{algpseudocode}

\usepackage{listings}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{siunitx}
\usepackage{booktabs}
\newtheorem{theorem}{Theorem}

\lstset{
  language=R,
  basicstyle=\ttfamily\small,
  keywordstyle=\color{blue},
  commentstyle=\color{gray},
  stringstyle=\color{red},
  numbers=left,
  numberstyle=\tiny,
  stepnumber=1,
  numbersep=8pt,
  frame=single,
  breaklines=true,
  showstringspaces=false
}


<<plots, child="plots.Rnw",  include=FALSE>>=
@

\usepackage[margin=2.5cm]{geometry}
\newcommand{\email}[1]{ (\href{mailto:#1}{#1})}
\usepackage{amsmath}
\begin{document}
\setcounter{secnumdepth}{0}


\title{Project 1, TMA4300}
\author{\email{mihan@stud.ntnu.no},  \email{alessatc@stud.ntnu.no}}
\maketitle

\tableofcontents
\newpage
\section{Introduction}
This report presents Project 1 for the course TMA4300 – Computer Intensive Statistical Methods, taught by Professor Jarle Tufto at NTNU.

Throughout the project, we make extensive use of the random number generators provided by the R programming language. In particular, we adopt the Mersenne–Twister generator due to its very long period, good equidistribution properties, and widespread use in statistical computing. To ensure full reproducibility of the results, the random seed is fixed.

<<generator, fig.width=5, fig.height=4, out.width='0.7\\textwidth'>>=
RNGkind(kind = "Mersenne-Twister"); # Use Mersenne-Twister generator
set.seed(9);                        # set seed for reproducibility
@

All plots are generated through specific functions and collected in the section "Plot Functions".

\section{Problem 1}
\subsection{Point a)}
Consider the random variable $X$ with density
\begin{equation}
    f(x)=\frac{1}{\pi(1+x^2)} \quad \text{for}\  -\infty < x < \infty
\end{equation}
The CDF of $X$ can be computed as
\begin{equation}
    F_X(k) = \int_{-\infty}^kf(x)\ \mathrm dx= \int_{-\infty}^k\frac{1}{\pi(1+x^2)}\ \mathrm{d}x = \frac{1}{\pi}\arctan (k)+\frac 12
\end{equation}
Using the inversion method, a sample from $X$ can be generated by solving
\begin{equation}
    u=\frac{1}{\pi}\arctan (k)+\frac 12
\end{equation}
\begin{equation}
    \implies k=\tan\Big[\pi(u-\frac12)\Big]
\end{equation}
where $U\sim Unif(0,1)$

<<problem1a1, fig.width=5, fig.height=4, out.width='0.7\\textwidth'>>=
u <- runif(10000); # generate the uniform distributed samples
x <- tan((u-1/2)*pi); # apply transformation

@
We can now proceed with the sanity check. Since the distribution is Cauchy, we can visualize the generated samples using a histogram and overlay the theoretical density:
<<problem1a2plot, fig.width=5, fig.height=4, out.width='0.7\\textwidth', fig.align='center'>>=
plotcauchyzoom(x, primary, secondary)
@

\subsection{Point b)}
Let \(X\) be a continuous random variable with probability density function \(f_X(x)\). If
\[
\mathbb{E}[|X|] = \int_{-\infty}^{\infty} |x| \, f_X(x) \, dx < \infty,
\]
then the expected value of \(X\) is defined as
\[
\mathbb{E}[X] = \int_{-\infty}^{\infty} x \, f_X(x) \, dx.
\]
Since
\begin{equation}
\mathbb{E}[|X|] = \int_{-\infty}^{\infty} |x| \, f_X(x) \, dx
= \int_{-\infty}^{\infty} \frac{|x|}{\pi(1+x^2)} \, dx
= 2 \int_0^\infty \frac{x}{\pi(1+x^2)} \, dx
\sim \int_0^\infty \frac{1}{x} \, dx \to \infty,
\end{equation}
the mean \(\mathbb{E}[X]\) \text{does not exist}.


\subsection{Point c)}
Since we proved that the exprected value does not exists (Point b), the strong law of large numbers can't be applied.
Assuming $X_i\overset{\text{iid}}{\sim} Cauchy(0,1)$. Since $\varphi_{X_i}(t)=e^{-\vert t \vert }$, we have
\begin{equation}
    \varphi_{X_1+\dots+X_n}(t) = (e^{-\vert t \vert })^n
\end{equation}
\begin{equation}
    \implies X_1+\dots+X_n =Y\sim Cauchy(0,n)
\end{equation}
\begin{equation}
    \varphi_{\frac 1nY}(t) = e^{-n\vert \frac 1n t\vert}=e^{-\vert t\vert} \implies \frac1n Y \sim Cauchy(0,1) \quad \forall n
\end{equation}
We can see that the mean continues to be a Cauchy-distributed for all $n$.
Thus,
\begin{equation}
    \mathbb P\Big (\Big\vert \frac 1n Y-c\Big \vert \geq \varepsilon\Big )=1-\int_{c-\varepsilon}^{c+\varepsilon} \frac{1}{\pi(1+x^2)}\ \mathrm dx  \overset{n\to \infty}{\longarrownot\longrightarrow }0
\end{equation}

This implies that the cumulative mean does not converge in probability to any constant (thus not in law, nor almost surely).
This explains the behaviour we see in the graph below: 

<<problem1b, fig.width=5, fig.height=4, out.width='0.7\\textwidth', fig.align='center'>>=

cummean <- function(x) {
  cumsum(x) / (1:length(x))
}
plotcumulativemean(cummean)

@
As $n$ increases, the mean doesn't converge to any constant value.


\section{Problem 2}
\subsection{Point a)}
\begin{algorithm}
\caption{Left Right Truncated Random Variable Generator}
\begin{algorithmic}[1]
\Require
$F$ \Comment{Cumulative Distribution Function of $X$}
\Require
$F^{-1}$ \Comment{Quantile function of $X$}
\Require
$a$ \Comment{Left truncation point (default $-\infty$)}
\Require
$b$ \Comment{Right truncation point (default $\infty$)}
\Require
$n$ \Comment{Number of samples}

\Ensure
$X_1, \dots, X_n \sim X \mid a \le X \le b$ \Comment{Generated samples}

\State Compute $\text{lb} \gets F(a)$
\State Compute $\text{ub} \gets F(b)$

\For{$i = 1$ to $n$}
  \State Draw $U_i \sim \mathcal{U}(0,1)$
  \State Set $\tilde{U}_i \gets  U_i \cdot \text{lb}+ (1-U_i)\cdot \text{ub}$
  \State Set $X_i \gets F^{-1}(\tilde{U}_i)$
\EndFor

\State \Return $(X_1, \dots, X_n)$
\end{algorithmic}
\end{algorithm}

\subsection{Point b)}
Function implementation:
<<problem2b1, fig.width=5, fig.height=4, out.width='0.7\\textwidth', fig.align='center'>>=

rtrunc <- function(n, pfn, qfn, a = -Inf, b = Inf, ...) {
  # calculate lower and upper bounds
  bound.lower <- do.call(pfn, c(list(a), list(...)))
  bound.upper <- do.call(pfn, c(list(b), list(...)))
  
  # generate uniformly randomly distributed samples in the provided interval 
  u <- runif(n)
  u_trunc <- bound.lower*u + (1-u)*bound.upper
  
  # calculate and return the transformed x
  return(do.call(qfn, c(list(u_trunc), list(...))))
}
@
Testing with the Binomial distribution truncated for $X\leq 10$
<<problem2b2, fig.width=5, fig.height=4, out.width='0.7\\textwidth', fig.align='center'>>=

x <- rtrunc(1e+3, pbinom, qbinom, b=10, size=20, prob=.5)
rtrunchistbinom(x)

@
\subsection{Point c)}
We can try applying the same function to the Normal distribution truncating it from -1 to 2.
<<problem2c1, fig.width=5, fig.height=4, out.width='0.6\\textwidth', fig.align='center'>>=

x <- rtrunc(1e3, pfn = "pnorm", qfn = "qnorm", a = -1, b = 2, mean = 0, sd = 1)
rtrunchistnorm(x)
@
<<problem2c2, fig.width=5, fig.height=4, out.width='0.6\\textwidth', fig.align='center'>>=

x <- rtrunc(1e3, pfn = "pexp", qfn = "qexp", a = 0.5, b = 3, rate = 1)
rtrunchistexp(x)

@
As shown in the graph above, the function works correctly: the histogram of the simulated samples matches the theoretical distribution within the selected interval.


\section{Problem 3}
\subsection{Point a)}

\begin{algorithm}[H]
\caption{Bivariate Normal Sampler Truncated to a Disk}
\begin{algorithmic}[1]
\Require
$n$ \Comment{Number of samples}
\Require
$\rho$ \Comment{Radius of the truncation disk}
\Require
$\delta$ \Comment{Shift along the $x$-axis}

\Ensure
$\text{samples} \in \mathbb{R}^{n \times 2}$ \Comment{Generated samples}

\State Draw $\theta_1, \dots, \theta_n \sim \mathcal{U}(0, 2\pi)$
\State Compute $u_\text{max} \gets \exp(-\rho^2 / 2)$
\State Draw $u_1, \dots, u_n \sim \mathcal{U}(u_\text{max}, 1)$
\State Compute $r_i \gets \sqrt{-2 \log(u_i)}$ for $i = 1,\dots,n$
\State Compute $x_{1,i} \gets r_i \cos(\theta_i) + \delta$
\State Compute $x_{2,i} \gets r_i \sin(\theta_i)$
\State \Return $\text{samples} \gets \{(x_{1,i}, x_{2,i})\}_{i=1}^n$
\end{algorithmic}
\end{algorithm}

Implementation:

<<problem3a, fig.width=6, fig.height=3, out.width='0.85\\textwidth', fig.align='center'>>=
# Truncated bivariate normal sampler
bnormsampler <- function(n, rho, delta) {
  theta <- runif(n, 0, 2 * pi)
  u_max <- exp(-rho^2 / 2)
  u <- runif(n, u_max, 1)
  r <- sqrt(-2 * log(u))
  x1 <- r * cos(theta) + delta
  x2 <- r * sin(theta)
  cbind(x1, x2)
}

# Parameters
n <- 1000
rho <- 3
delta <- 2

samples <- bnormsampler(n, rho, delta)
scatterplotdisk(samples, rho, delta)

@
No samples are rejected, so the acceptance rate is exactly 100\%, independent of the disk radius $\rho$ or the shift $\delta$.
\subsection{Point b)}
<<problem3b, fig.width=5.5, fig.height=4, out.width='0.8\\textwidth', fig.align='center'>>=
gbnormsampler <- function(n, mean1, mean2, std, rho, c1, c2) {

  samples <- matrix(NA, nrow = n, ncol = 2)
  count <- 0
  
  while(count < n) {
    # Generate candidate points
    u <- runif(1,0,1)
    theta <- runif(1, 0, 2*pi)
    r <- sqrt(-2*log(u))
    x <- r*cos(theta)*std + mean1
    y <- r*sin(theta)*std + mean2
    
    # Check if inside the truncation disk
    if((x - c1)^2 + (y - c2)^2 <= rho^2) {
      count <- count + 1
      samples[count, ] <- c(x, y)
    }
  }
  return(samples)
}

# Parameters
n <- 1000
mean1 <- -3
mean2 <- -1
std <- 2.9
rho <- 4
c1 <- 4
c2 <- 3

samples <- gbnormsampler(n, mean1, mean2, std, rho, c1, c2)
gbnormamplerplotter(samples, mean1, mean2, rho, c1, c2)


@



\section{Problem 4}

\subsection{Point a)}
In this problem, we have $n$-uniformaly distributed random variables $U_1, \ldots, U_n \sim unif(0,1)$ from which we can get the joint density of minimum and maximum order statistics $U_{(1)}$ and $U_{(n)}$ using formula:
$$f_{U_{(i)}, U_{(j)}}(u,v) = n! \cdot \frac{u^{i-1}}{(i-1)!} \cdot \frac{(v-u)^{j-i-1}}{(j-i-1)!} \cdot \frac{(1-v)^{n-j}}{(n-j)!} \quad \text{for } 0 < u < v < 1.$$
In our case we have $i = 1$ and $j = n$ so the joint density is:
$$f_{U_{(1)}, U_{(n)}}(u,v) = n(n-1)(v-u)^{n-2}.$$
Our goal is to get the density $f_X(x)$ of random variable 
$$X = \frac{\frac{1}{2} (U_{(1)} + U_{(n)}) - \frac{1}{2}}{U_{(n)} - U_{(1)}}.$$
First, we set $$Y = h(U_{(1)}, U_{(n)}) = U_{(n)} - U_{(1)}$$
and from expression for $X$ we get that
$$\frac{1}{2} (U_{(1)} + U_{(n)}) = X \cdot Y + \frac{1}{2}.$$
We can now express $U_{(1)}$ and $U_{(n)}$ using expressions for $X$ and $Y$:
$$U_{(1)} = \frac{1}{2} + XY - \frac{Y}{2}$$
and
$$U_{(n)} = \frac{1}{2} + XY + \frac{Y}{2}.$$
Now we want to compute Jacobian determinant based on the inverse transformation, therefore we first compute partial derivatives:
$\frac{\partial U_{(1)}}{\partial X} = Y, \quad \frac{\partial U_{(1)}}{\partial Y} = X - \frac{1}{2}, \quad \frac{\partial U_{(n)}}{\partial X} = Y \text{ and } \quad \frac{\partial U_{(n)}}{\partial Y} = X + \frac{1}{2}.$
We compute Jacobian determinant as:
\vspace{6pt}
$$|J| = \begin{vmatrix}
Y & X - 1/2 \\ 
Y & X + 1/2
\end{vmatrix} = Y.$$
Joint density is there calculated using transformation formula and definition of $X$ and $Y$. We can use tranformation formula sicer we have a one-to-one differentiable function with inverse, as shown above.
\begin{align*}
    f_{X,Y}(x,y) &= f_{U_{(1)}, U_{(n)}} (u,v) \cdot |J| \\
    &= n(n-1)(v-u)^{n-2} \cdot |Y| \\
    &= n(n-1) y^{n-2} \cdot y\\
    &= n(n-1) y^{n-1}.
\end{align*}
We now get density of $X$ by integrating over $Y$. We get the boundaries for integration from constrait $0 < U_{1} < U_{n} < 1$ from which we get that $y < \frac{1}{1 - 2x}$ and $y < \frac{1}{1 + 2x}$. We therefore integrate over $y$ from 0 to $\frac{1}{1 + 2|x|}$. Hence,

\begin{align*}
    f_X(x) &= \int_{y = 0}^{\frac{1}{1 + 2 |x|}} n(n - 1)y^{n - 1}dy \\
    &= (n - 1) \left( \frac{1}{1 + 2 |x|} \right)^n.
\end{align*}

\newpage

\subsection{Point b)}
We now integrate our proposed algorithm into R as seen below:
<<problem4b, fig.width=6, fig.height=3, out.width='0.85\\textwidth', fig.align='center'>>=

#Problem 4
# We decide to take 100 random uniformally distributed random variables, so we take U_{1} and U_{100}
n <- 100
num_simulations <- 1000 #number of samples

theoretical_fX <- function(x, n) {
  (n - 1) * (1 / (1 + 2 * abs(x))^n)
}

set.seed(42) 
x_samples <- replicate(num_simulations, {
  u <- runif(n)
  u_min <- min(u)
  u_max <- max(u)
  x_val <- (0.5 * (u_min + u_max) - 0.5) / (u_max - u_min)
  return(x_val)
})

x_limit <- 0.075

hist(x_samples, breaks = 50, probability = TRUE, 
     main = paste("Empirical vs Theoretical (n =", n, ")"),
     xlab = "x", xlim = c(-x_limit, x_limit),
     col = "lightblue", border = "white")

# Add theoretical curve
curve(theoretical_fX(x, n), add = TRUE, col = "red", lwd = 2, n = 1000)

@
\noindent Based on graph we conclude that the calculated density function $f_X(x)$ is correct as the curve fits the simulated samples. 

\section{Problem 5}

\subsection{Point a)}
The aim of this problem is to show how we would simulate data from joint distribution of random variables X and Y. We are given joint distribution:
$$F_{X,Y}(x,y) = \exp{\left(-\frac{1}{x} - \frac{1}{y} - \frac{1}{xy}\right)} \quad \text{for } x>0, y>0.$$
First, we get the marginal cumulative distribution function of X:
$$F_X(x) = \lim_{y \rightarrow \infty} F_{X,Y}(x,y) = \exp{\left(-\frac{1}{x}\right)} \quad \text{for } x > 0.$$

\noindent We want to get cdf of $Y|X = x$ from formula
$$
F_{Y|X}(y|x) = \frac{1}{f_X(x)} \frac{\partial}{\partial x} F_{X,Y}(x,y).
$$
First, we show theoreticaly that the formula holds so we can use it later. We can write righthand side as 
\begin{align*}
    RS &= \frac{1}{f_X(x)} \frac{\partial}{\partial x} F_{X,Y}(x,y)\\
    &= \frac{1}{f_X(x)} \frac{\partial}{\partial x} \left(\int_{-\infty}^{x}\int_{-\infty}^{y}f_{X,Y}(u,v) dv du\right)\\
    &= \frac{1}{f_X(x)} \left(\int_{-\infty}^y f_{X,Y}(x,v) dv\right),
\end{align*}
Which follows from fundamental theorem of calculus. We rewrite lefthand side using formula $f_{Y|X}(y|x) = \frac{f_{X,Y}(x,y)}{f_X(x)}$;
\begin{align*}
    F_{Y|X}(v|x) &= \int_{-\infty}^{y} f_{Y|X}(v|x) dv \\
    &= \int_{-\infty}^{y} \frac{f_{X,Y}(x,y)}{f_X(x)} dv \\
    & = \frac{1}{f_X(x)} \left(\int_{-\infty}^y f_{X,Y}(x,v) dv\right),
\end{align*}
and we see that both sides are the same. The same obviously holds for our case of $x > 0$ and $y > 0$. We calculate what is needed:
$$
\frac{\partial}{\partial x} F_{X,Y}(x,y) = exp\left(-\frac{1}{x} -\frac{1}{y} - \frac{1}{xy}\right) \cdot \left(\frac{1}{x^2} + \frac{1}{x^2 y}\right) \quad \text{for } x > 0, y>0
$$
$$
f_X(x) = \frac{\partial F_X(x)}{\partial x} = \frac{1}{x^2} \cdot \exp{\left(-\frac{1}{x}\right)} \quad \text{for } x > 0.
$$

\noindent Using previous formula conditional cdf is
\begin{align*}
    F_{Y|X}(y|x) &= \frac{1}{f_X(x)} \cdot \frac{\partial}{\partial x} F_{X,Y}(x,y) = \\
    &= \frac{1}{\frac{1}{x^2} \exp{\left(-\frac{1}{x}\right)}} \cdot \exp\left(-\frac{1}{x} -\frac{1}{y} - \frac{1}{xy}\right) \cdot \left(\frac{1}{x^2} + \frac{1}{x^2 y}\right)\\
    &= \exp{\left(-\frac{1}{y}-\frac{1}{xy}\right)} \cdot \left(1 + \frac{1}{y}\right).
\end{align*}

\noindent From this, after another derivation it quickly follows that
\begin{align*}
    f_{Y|X}(y|x) &= \frac{\partial}{\partial y} F_{Y|X}(y|x) \\
    &= \exp\left(-\frac{1}{y} - \frac{1}{xy}\right) \cdot \left(\frac{1}{xy^2} + \frac{1}{y^3} + \frac{1}{xy^3}\right)
\end{align*}

\noindent We also compute density function $f_{X,Y}(x,y)$ as:
\begin{align*}
    f_{X,Y}(x,y) &= \frac{\partial^2}{\partial x}{\partial y} F_{X,Y}(x,y) \\
        &= \exp\left(-\frac{1}{x} - \frac{1}{y} -\frac{1}{xy}\right) \cdot \left(\frac{1}{x^2y^3} + \frac{1}{x^3y^2} + \frac{1}{x^3}{y^3}\right).
\end{align*}

\vspace{12pt}
\noindent Recall that density of inverse Gamma distributed random variable is defined as
$f(y, \alpha, \beta) = \frac{\beta^\alpha}{\Gamma(\alpha)} y^{-\alpha - 1} \exp{\left(-\frac{\beta}{x}\right)}$ for $x > 0, \, \alpha > 0,\, \beta > 0.$From density $f_{Y|X}(y|x)$ we recognize that $Y|X = x$ is a mixture of two independant random variables that have inverse Gamma distribution. By rewriting the expression, we recognize that $\beta = \frac{x + 1}{x}$, $w_1 = \frac{1}{1 + x}$, $w_2 = \frac{x}{1 + x}$, $\alpha_1 = 1$ and $\alpha_2 = 2$ and the density is then written as:
\begin{align*}
    f_{Y|X}(y|x) &= \exp\left(-\frac{1}{y} - \frac{1}{xy}\right) \cdot \left(\frac{1}{xy^2} + \frac{1}{y^3} + \frac{1}{xy^3}\right) \\
    & = \frac{1}{x+1} \cdot \frac{\frac{x+1}{x})^{1}}{\Gamma(1)} y^{-2} \cdot \exp{\left(-\frac{1 + 1/x}{y}\right)} \\
    &  + \frac{x}{x+1} \cdot \frac{(\frac{x + 1}{x})^{2}}{\Gamma(2)} y^{-3} \cdot \exp{\left(-\frac{1 + 1/x}{y}\right)},
\end{align*}
Therefore, we can write conditional cdf as 
$$Y|X = x \sim \frac{1}{x+1} \cdot IG\left(1, \frac{x + 1}{x}\right) + \frac{x}{1 + x} \cdot IG\left(2, \frac{x + 1}{x}\right).$$

\vspace{12pt}
\noindent We can now simulate data from joint distribution of X and Y by first simulating X. This is done by inverse method via random variable $U \sim unif(0,1)$ and as $U = F_X(x)= 1 - \exp(-1/x)$, we have $X = -\frac{1}{\ln(U)}$. Next we simulate $Y|X = x$ as a mixture of two inverse Gamma distributions.
First, we decide to define a new Bernoulli random variable Z with parameter $p = \frac{x}{x+1}$; if $Z = 1$ we simulate Y from $IG\left(1, \frac{x + 1}{x}\right)$ and if $Z = 0$ we simulate Y from $IG\left(2, \frac{x + 1}{x}\right)$. Implemented algorithm is shown in R code below.

\subsection{Point b)}
We therefore simulate our empirical cdf using the algorithm below, written in R:

<<problem5b1, fig.width=8, fig.height=5, out.width='0.85\\textwidth', fig.align='center'>>=

n_samples <- 50000  
set.seed(123)

given_fXY <- function(x, y) {
  exp(-1/x - 1/y - 1/(x*y)) * (1/(x^2 * y^3) + 1/(x^3 * y^2) + 1/(x^3 * y^3))
}

u <- runif(n_samples)
x_sim <- -1 / log(u)

y_sim <- numeric(n_samples)
for (i in 1:n_samples) {
  x <- x_sim[i]
  beta <- (x + 1) / x
  if (runif(1) < (x / (x + 1))) {
    y_sim[i] <- 1/rgamma(1, shape = 2, rate = beta)
  } else {
    y_sim[i] <- 1/rgamma(1, shape = 1, rate = beta)
  }
}

@

\noindent We provide sanity check by plotting joint density $f_{X,Y}(x,y)$ and 100000 samples generated by a previous algorithm.


<<problem5b2, eval=FALSE, echo=TRUE, fig.width=8, fig.height=5>>=
install.packages("plot3D")
library(plot3D)
grid_limit <- 5

grid_seq <- seq(0.1, grid_limit, length.out = 50)
z_theory <- outer(grid_seq, grid_seq, given_fXY)

persp(grid_seq, grid_seq, z_theory, 
      theta = 35, phi = 30, expand = 0.5, 
      col = "lightblue", shade = 0.5,
      main = "Given joint density",
      xlab = "x", ylab = "y", zlab = "",
      xlim = c(0,5), ylim = c(0,5),
      ticktype = "detailed", nticks = 5)

num_bins <- 15
x_cuts <- seq(0, grid_limit, length.out = num_bins + 1)
y_cuts <- seq(0, grid_limit, length.out = num_bins + 1)
z_counts <- table(cut(x_sim, x_cuts), cut(y_sim, y_cuts))

hist3D(z = as.matrix(z_counts), x = x_cuts[-1], y = y_cuts[-1],
       theta = 35, phi = 30, expand = 0.5,
       col = "lightgreen", border = "black", 
       space = 0.1,
       main = paste("Simulation (50000 samples)"),
       xlab = "x", ylab = "y", zlab = "", 
       xlim = c(0,5), ylim = c(0,5),
       ticktype = "detailed", nticks = 5)
@



\begin{center}
\includegraphics[width=0.7\linewidth]{Density5.jpg}
\includegraphics[width=0.8\linewidth]{Sample5.jpg}
\end{center}

\noindent On first graph, $z$ axis represent probability and on second one number of samples from which probability can be computed as well. We are satisfied with the proposed algorithm as the graphs look similar.

\section{Problem 6}

\subsection{Point a)}
In this problem, we want simulate data from distribution of random variable $Y$ with density which omites the normalizing constant:
$$
f^*(y) = \begin{cases}
\left(\sqrt{y^2 +4} - y\right)^2 & y \geq 0,\\
0 & y <0.
\end{cases}
$$

\noindent We decide to use a ratio - of - uniforms method. We first show that both $f^*(y)$ and $y^2 f^*(y)$ are bounded.
We first rewrite 
$$\sqrt{y^2 + 4} - y = \frac{4}{\sqrt{y^2 + 4} + y}$$ 
and therefore
$$f^*(y) = \frac{16}{(\sqrt{y^2 + 4} + y)^2} \quad \text{for } y \geq 0.$$
We see that the function $f^*(y)$ is bounded with value 4 in $y = 0$ and in limit, function behaves as $f^*(y) \sim \frac{4}{y^2}$ and $\lim_{y \to \infty} f^*(y) = 0$. 
We also analyze the function $y^2 f^*(y)$:
$$y^2 f^*(y) = y^2 \cdot \left(\frac{4}{\sqrt{y^2 + 4} + y}\right)^2 = \frac{16 y^2}{y^2 + 4 + 2y\sqrt{y^2 + 4}} \quad \text{for } y \geq 0.$$
From behavuour of previous expression in the limit we quickly see that $$\lim_{y \rightarrow \infty} y^2 f^*(y) = 4.$$
As it is also bounded in $y = 0$ with value 0, we conclude that $y^2 f^*(y)$ is also bounded. The conditions are therefore satisfied.

\vspace{12pt}
\noindent We continue with simulating $(X_1, X_2) \sim Unif(C)$, where
$$C = \{(x_1, x_2) \in \mathbb{R}^2 : 0 < x_1 \leq \sqrt{f^*(x_2/x_1)}, \quad 0 < x_2 \leq x_1 \sqrt{f^*(x_2/x_1)}\}.$$
We have to emphasize that we accept if $X_1 \leq f^*(X_2/x_1)$ which also implies that there will be some rejections and rejection probability to calculate.


\noindent We calculate bounds of set $C \subset [0,a] \times [b^-, b^+]$, where
\begin{align*}
a & = \sup_{y \geq 0} \sqrt{f^*(y)} = \sqrt{4} = 2, \\
b^- & = -\sqrt{\sup_{y \leq 0} y^2 {f^*(y)}} = 0 \\
b^+ &= \sqrt{\sup_{y \geq 0} y^2 f^*(y)} = \sqrt{4} = 2.
\end{align*} 

\vspace{12pt}
\noindent Proposed algorithm goes as follows. We first sample $X_1 \sim Unif(0,2)$ and $X_2 \sim Unif(0,2)$, accept if $X_1^2 \leq f^*(X_2/X_1)$ and for accepted $(X_1, X_2)$ set $Y = X_2/X_1$ which is our simulated data from desired distribution. We plot 10000 samples of normalized $f(\frac{x_2}{x_1}).$ 

\vspace{12pt}
\noindent Before moving onto the R code we have to compute normalization constant for $f^*(y)$ so it integrates to 1. By integration we get that 
$$\int_{0}^{\infty} \left(\sqrt{y^2 + 4} - y \right)^2 = \frac{16}{3},$$
so the constant is $\frac{3}{8}$ and our density function we want to plot is:
$$f(x) = \frac{3}{16} \left(\sqrt{y^2 + 4} - y \right)^2.$$

\subsection{Point b)}
We implement algorithm described above in R:


<<problem6b1, fig.width=5, fig.height=4, out.width='0.7\\textwidth'>>=
set.seed(42)
n_sim <- 10000
total_proposals <- 0

f_star <- function(y) {
  ifelse(y >= 0, (sqrt(y^2 + 4) - y)^2, 0)
}

simulate_y <- function(n) {
  y <- numeric(0)
  while (length(y) < n) {
    m <- max(1000, n - length(y))  
    total_proposals <<- total_proposals + m
    u <- runif(m, 0, 2)
    v <- runif(m, 0, 2)   
    y_prop <- v / u
    accept <- (u^2 <= f_star(y_prop))
    y <- c(y, y_prop[accept])
  }
  return(y[1:n])
}

y_samples <- simulate_y(n_sim)
f_normalized <- function(y) (3/16) * f_star(y)

curve(f_normalized(x), 
      from = 0, 
      to = 10,            
      col = "red", 
      lwd = 2, 
      main = "Function f(x)", 
      xlab = "y", 
      ylab = "Density", 
      ylim = c(0, 1))

hist(y_samples[y_samples <= 10],
     breaks = seq(0, 10, by = 0.5),
     probability = FALSE,
     col = "skyblue",
     border = "white",
     main = "Simulation (10000 samples)",
     xlab = "Sample values (y)",
     ylab = "Number of samples",
     xlim = c(0, 10))

emp_prob <- n_sim / total_proposals
cat("Empirical Acceptance Probability:", emp_prob, "\n")
@

\noindent We are satisfied with our resu,t as it seems like simulation fits the density function. We limited ourselves to values from 0 to 10 as the probability of samples with higher numbers than that is quite low and the graph is more readable that way. We have calculated acceptance probability empiricaly since we counted number of total tries of sampling and number of accepted samples and got the probability of 0.65 percent.

\section{Problem 7}
We have a normal distributed random variable $X \sim N(\mu, \sigma^2)$ and logit-normal distributed random variable $Y = \frac{1}{1 + e^{-X}}$.

\subsection{Point a)}
We write $Y = h(X) = \frac{1}{1 + e^{-X}}$ and compute a Monte-Carlo estimate of $E(h(X))$ based on $n = 1000$ realizations $x_1, \ldots, x_n$ of $X$ by i.i.d. simulating $x_1, \ldots, x_n \sim f(x)$, where f(x) is density of our normal distriburion, and then using formula:
$$\widehat{E(h(X))} = \frac{1}{n} \sum_{i=1}^n h(x_i).$$

\noindent We use R code:
<<problem7a, fig.width=5, fig.height=4, out.width='0.7\\textwidth'>>=
set.seed(123)
n <- 1000
mu <- 1
sigma <- 1
h <- function(x) 1 / (1 + exp(-x))
x <- rnorm(n, mean = mu, sd = sigma)
h_x <- h(x)
mc_estimate <- mean(h_x)
se_mc <- sd(h_x) / sqrt(n)
cat("MC Estimate:", mc_estimate, "\nSE:", se_mc)
@
\noindent Our estimated $E(h(X))$ is 0.6996 with standard error of 0.05696.


\subsection{Point b)}
We produce additional $n$ samples $x_1^*, \ldots, x_n^*$ by antithetic sampling and computed the wanted estimate as proposed by R code:

<<problem7b, fig.width=5, fig.height=4, out.width='0.7\\textwidth'>>=
z <- rnorm(n)
x <- mu + sigma * z
x_star <- mu - sigma * z
h_combined <- (h(x) + h(x_star)) / 2
antithetic_estimate <- mean(h_combined)
se_anti <- sd(h_combined) / sqrt(n)
correlation <- cor(h(x), h(x_star))
cat("Antithetic Estimate:", antithetic_estimate, "\nSE:", se_anti, "\nCorr:", correlation)
@
\noindent Our estimated $E(h(X))$ is again 0.6960 but with smaller standard error of 0.001216. We compute the standard error by treating each pair as a single independent observation $\hat{Y_i}$, so $\hat{Y_i} = \frac{h(x_i) + h(x_i^*)}{2}$. Since the pairs are independent, we can apply the standard se formula to these n averages: $se = \frac{sd(\hat{Y})}{\sqrt{n}}$. We also know that both $h(x_i)$ and $h(x_i^*)$ have same variance and all samples are i.i.d.  We therefore compute variance of estimator as

\begin{align*}
Var(\hat{\mu}_b) &= \frac{Var(h(X) + h(X^*))}{4n} \\
&= \frac{1}{2n} \left(Var(h(X)) + Cov(h(X), h(X^*))\right) \\
&= \frac{Var(h(X))}{2n} \left(1 + Corr(h(X), h(X^*))\right).
\end{align*}

\noindent We get standard deviation as a square root of the variance. We computed that $Corr(h(X),h(X^*)) = - 0.91$, therefore less than zero which explains why the standard error is lower than in first part of the problem.

\subsection{Point c)}
We first need to set the parameters of normal distribution that will help us with importance sampling. First, we plot $f(x)$ and $h(x)$ on graph below:

<<problem7c1, fig.width=5, fig.height=4, out.width='0.7\\textwidth'>>=
f <- function(x) dnorm(x, 1, 1)
h <- function(x) 1 / (1 + exp(-x))
curve(f(x), -3, 5, col="blue", lwd=2, ylab="Density", main="Functions f(x) and h(x)",ylim = c(0, 1))
curve(h(x), -3, 5, col="darkgreen", lwd=2, add=TRUE)
legend("topleft", legend=c("f(x)", "h(x)"),
       col=c("blue", "darkgreen"), lwd=2)
@

\noindent Now, we will plot function $f(x)*h(x)$ and then try different parameters for normally distributed $g(x)$ to find one that suits it the most. 

<<problem7c2, fig.width=5, fig.height=4, out.width='0.7\\textwidth'>>=
target_prod <- function(x) f(x) * h(x)
curve(target_prod, -3, 5, col="blue", lwd=2, ylab="Density", main="Choosing g(x)",ylim = c(0, 0.5))

g_mu <- 1.2
g_sigma <- 0.95
curve(dnorm(x, g_mu, g_sigma), -3, 5, col="red", add=TRUE)
legend("topleft", legend=c("h(x)f(x)", "g(x)"),
       col=c("blue","darkgreen"), lwd=2)
@

\noindent On graph, we see the proposed $g(x)$ we are going to use for importance sampling as it seemed to give the best results, also in comparison to results in part e. We implement in R importance sampling algorithm as we did in lectures, simulating i.i.d. $x_1, x_2, \ldots x_n \sim g(x)$ and computing
$$
\widehat{E(h(X))} = \frac{1}{n} \sum_{i=1}^n h(x_i) \frac{f(x_i)}{g(x)_i}.
$$
       
<<problem7c3, fig.width=5, fig.height=4, out.width='0.7\\textwidth'>>=
set.seed(123)
n <- 1000
x_c <- rnorm(n, g_mu, g_sigma)
w_c <- (h(x_c) * f(x_c)) / dnorm(x_c, g_mu, g_sigma)
is_estimate <- mean(w_c)
se_is <- sd(w_c)/sqrt(n)
cat("Importance Sampling Estimate:", is_estimate, "\nSE:", se_is, "\n")
@

\noindent We see that that the estimation of mean stayed accurate and standard error got just a bit worse compared to anthetic sampling method.

\subsection{Point d)}
Now we combine both previously used method, same importance sampling as in part c but just with $2n$ samples as in part b:
<<problem7d, fig.width=5, fig.height=4, out.width='0.7\\textwidth'>>=
z <- rnorm(n/2)
y1 <- g_mu + g_sigma * z
y2 <- g_mu - g_sigma * z 
w1 <- target_prod(y1) / dnorm(y1, g_mu, g_sigma)
w2 <- target_prod(y2) / dnorm(y2, g_mu, g_sigma)
combined_samples <- (w1 + w2) / 2
combined_estimate <- mean(combined_samples)
se_combined <- sd(combined_samples) / sqrt(n/2)

combined_function <- function(x) target_prod(x) / dnorm(x, g_mu, g_sigma)
curve(combined_function, -2, 4, main="Combined function", ylab="f(x)*h(x)/g(x)", xlab="x", ylim = c(0.2,0.9))

cat("Combined Estimate:", combined_estimate, "\nSE:", se_combined, "\n")
@

\noindent On graph, we see the function $\frac{f(x) * h(x)}{g(x)}$. It is monotone which is wanted because then the samples of antithetic pair will be negativelly correlated which will lower overall variance. This showed up to be correct as we see that while estimation of mean stayed the same, standard error got lower.


\subsection{Point e)}

<<problem7e, fig.width=5, fig.height=4, out.width='0.7\\textwidth'>>=
true_val <- integrate(target_prod, lower = -Inf, upper = Inf)$value

results <- data.frame(
  Method = c("Monte Carlo (a)", "Antithetic (b)", "Importance (c)", "Combined (d)", "Numerical value (e)"),
  Estimate = c(mc_estimate, antithetic_estimate, is_estimate, combined_estimate, true_val),
  Std_Error = c(se_mc, se_anti, se_is, se_combined, 0)
)
print(results)
@
\noindent In the table above, we see that all methods returned similar and accurate estimates for mean. The estimator with smallest standard error seems to be the one, derived using both antithetic and importance sampling. 







\section{Problem 8}
\subsection{Point a)}
We used Cholesky factor decomposition of the variance matrix.
<<problem8a, fig.width=8, fig.height=4, out.width='0.8\\textwidth', fig.align='center'>>=

mvnchol <- function(size, n, mu, S) {
  L <- t(chol(S)) # compute Cholesky lower triangular
  u <- runif(size*n,0,1)
  theta <- runif(size*n,0,2*pi)
  r <- sqrt(-2*log(u))
  Z <- matrix(r*cos(theta), nrow = size, ncol = n)
  
  X <- L %*% Z + matrix(mu, nrow = size, ncol = n)
  
  return(X) 
}

# Parameters
size <- 2
n <- 5000
mu <- c(1, 2)
S <- matrix(
  c(2, 3,
    3, 10),
  nrow = 2, byrow = TRUE)

X <- mvnchol(size, n, mu, S)
plotMarginal2D(X, scattermain = "Scatterplot of bivariate normal distribution",
             scatterxlab = "X1",
             scatterylab = "X2")
@

\subsection{Point b)}

<<problem8b, fig.width=9, fig.height=5, out.width='0.8\\textwidth', fig.align='center'>>=
gausscopula <- function(n, std = c(1,1), mu = c(0,0), corr){
  cov <- corr*std[1]*std[2]
  S <- matrix(
    c(std[1]^2, cov,
      cov, std[2]^2),
    nrow = 2,
    byrow = TRUE
  )
  X <- mvnchol(2, n, mu, S)
  U <- pnorm(X)
  return(U)
}

truecorr <- 0.5 # correlation
n <- 1000
U <- gausscopula(n = n,corr = truecorr)
plotMarginal2D(U, scattermain = "Scatterplot of gauss copula",
             scatterxlab = "U1",
             scatterylab = "U2")
@
Since $X_1,X_2\sim\mathcal N(\mu,\mathbf S)$, $U_1=\Phi(X_1)$ and $U_2=\Phi(X_2)$ are uniformly distributed, as we can see in the marginal histograms above.
Moreover, they are dependent, which is evident from the scatterplot: the points tend to accumulate along the diagonal from (0,0) to (1,1), reflecting the positive correlation of the original Gaussian variables.

Monte Carlo estimation of correlation:
<<problem8b2, fig.width=9, fig.height=5, out.width='0.8\\textwidth', fig.align='center'>>=
gammamc <- function(U){
    n <- ncol(U)
    E1   <- mean(U[1,])
    E2   <- mean(U[2,])
    E12  <- mean(U[1,]*U[2,])
    E11  <- mean(U[1,]^2)
    E22  <- mean(U[2,]^2)
    cov12 <- E12 - E1*E2 # covariance
    var1  <- E11 - E1^2
    var2  <- E22 - E2^2
    gamma <- cov12 / sqrt(var1*var2)
    return(gamma)
}

print(gammamc(U))
@
\subsection{Point c)}
Given
\begin{equation}
    (X_1, X_2) \sim \mathcal N\Big (\begin{pmatrix}0\\0\end{pmatrix}, \begin{pmatrix}
        1 & \rho \\ \rho & 1
    \end{pmatrix}\Big ) \quad \quad \rho \in [-1,1]
\end{equation}
define the trasnformed variables $Y_1 = \Phi(X_1) , \ Y_2=\Phi(X_2)$.

\begin{equation}
    Y_1,Y_2\sim Uniform(0,1) \implies \mathbb E[Y_i]=\frac 12, \quad Var(Y_i)=\frac 1{12}
\end{equation}
Considering that $\Phi(x)=\mathbb P(Z\leq x) , \quad Z\sim\mathcal N(0,1)$, then
\begin{equation}
    \mathbb E[\Phi(X_1)\Phi(X_2)] = \mathbb P(Z_1\leq X_2, Z_2\leq X_2)
\end{equation}
Defining
\begin{equation}
    K_1=X_1-Z_1,\quad K_2=X_2-Z_2
\end{equation}
Then
\begin{equation}
    \mathbb P(Z_1\leq X_2, Z_2\leq X_2)=\mathbb P(K_1\geq 0, K_2\geq 0)
\end{equation}
Since $(X_1,X_2)$ is gaussian and $(Z_1,Z_2)$ is independent gaussian,
\begin{equation}
    (K_1,K_2)\sim\mathcal N
    \begin{pmatrix}
        \begin{pmatrix}
        0\\0
    \end{pmatrix}, \begin{pmatrix}
        2&\rho\\ \rho & 2
    \end{pmatrix}
    \end{pmatrix}
\end{equation}
The correlation of $(K_1,K_2)$ is therefore
\begin{equation}
    Corr(K_1,K_2)=\frac \rho 2
\end{equation}
For a centered bivariate normal $(U,V)$ with correlation $r$,
\begin{equation}
    \mathbb P(U\geq 0, V\geq 0) = \frac 14 +\frac 1{2\pi}\arcsin(r)
\end{equation}
Setting $r=\frac \rho 2$
\begin{equation}
    \mathbb E[\Phi(X_1)\Phi(X_2)]=\mathbb E[Y_1Y_2]=\frac 14+\frac{1}{2\pi}\arcsin \Big(\frac \rho 2\Big )
\end{equation}
\begin{equation}
    Cov(Y_1,Y_2)=\mathbb E[Y_1Y_2]-\mathbb E[Y_1]\mathbb E[Y_2] = \frac{1}{2\pi}\arcsin \Big(\frac \rho 2\Big )
\end{equation}
\begin{equation}
    \gamma=Corr(Y_1,Y_2)=\frac{Cov(Y_1,Y_2)}{1/12}=\frac 6\pi \arcsin\Big(\frac \rho 2\Big)
\end{equation}
\begin{equation}
    \implies \rho=2\sin\Big(\frac{\pi}{6}\gamma\Big)
\end{equation}
<<problem8c, fig.width=8, fig.height=5, out.width='0.8\\textwidth', fig.align='center'>>=
n_seq <- seq(2000, 100000, by = 1000)
gamma_mc <- numeric(length(n_seq))
adj_corr <- numeric(length(n_seq))
for(i in seq_along(n_seq)){
  n <- n_seq[i]
  U <- gausscopula(n = n, corr = truecorr)
  gamma_mc[i] <- gammamc(U)
  adj_corr[i] <- 2*sin(pi*gamma_mc[i]/6) # relation (21) in the text
}
adjustedcorrplot(n_seq, gamma_mc, adj_corr)
@
The plot above confirms the correctness of our results: as $n$ increases, the adjusted correlation $\rho$ approaches the true value more closely than the Monte Carlo estimate $\gamma$.
\subsection{Point d)}
<<problem8d, fig.width=8, fig.height=5, out.width='0.8\\textwidth', fig.align='center'>>=
U <- gausscopula(n=100000,corr=truecorr)
Y <- rbind(
  qbinom(U[1, ], 10, 0.25),
  qbinom(U[2, ], 10, 0.75)
)

plotMarginal2D(Y, scattermain = "Scatterplot of bivariate binomial",
             scatterxlab = "Y1",
             scatterylab = "Y2")

print(gammamc(Y))
@
The marginal distributions are just the binomial distributions.

Since $U_1$ and $U_2$ are not independent, also $Y_1$ and $Y_2$ are not independent.
That is also confirmed by the estimated correlation $\sim 0.4704 >0$ .


\section{Plot Functions}

<<prots, child="prots.Rnw">>=
@
\end{document}
